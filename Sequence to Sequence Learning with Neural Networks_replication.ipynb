{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "nous allons construire un modèle d’apprentissage automatique pour passer d’une séquence à l’autre, en utilisant PyTorch et torchtext. Cela se fera sur les traductions de l’allemand vers l’anglais, mais les modèles peuvent être appliqués à tout problème impliquant de passer d’une séquence à une autre, comme le résumé, c’est-à-dire passer d’une séquence à une séquence plus courte dans la même langue.\n",
    "\n",
    "Dans ce cahier, nous commencerons par comprendre les concepts généraux en implémentant le modèle de l’article Sequence to Sequence Learning with Neural Networks.\n",
    "\n",
    "\n",
    "## Préparation des données\n",
    "Nous allons coder les modèles dans PyTorch et utiliser torchtext pour nous aider à faire tout le prétraitement requis. Nous utiliserons également spaCy pour aider à la tokenisation des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons définir les graines aléatoires pour des résultats déterministes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous allons créer les tokenizers. Un tokenizer est utilisé pour transformer une chaîne contenant une phrase en une liste de jetons individuels qui composent cette chaîne, par exemple « bonjour! » devient [\"bon », « jour », « !\"]. Nous allons commencer à parler des phrases comme étant une séquence de jetons à partir de maintenant, au lieu de dire qu’elles sont une séquence de mots. Quelle est la différence ? Eh bien, « bon » et « matin » sont à la fois des mots et des jetons, mais « ! » est un jeton, pas un mot.\n",
    "\n",
    "spaCy a un modèle pour chaque langue (« de_core_news_sm » pour l’allemand et « en_core_web_sm » pour l’anglais) qui doivent être chargés afin que nous puissions accéder au tokenizer de chaque modèle.\n",
    "\n",
    "Remarque : les modèles doivent d’abord être téléchargés à l’aide des éléments suivants sur la ligne de commande :\n",
    "\n",
    "python -m spacy download en_core_web_sm\n",
    "python -m spacy download de_core_news_sm\n",
    "Nous chargeons les modèles comme tels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous créons les fonctions de tokenizer. Ceux-ci peuvent être transmis au texte torch et prendront la phrase comme une chaîne et retourneront la phrase comme une liste de jetons.\n",
    "\n",
    "Dans le document que nous mettons en œuvre, ils trouvent bénéfique d’inverser l’ordre de l’entrée qui, selon eux, « introduit de nombreuses dépendances à court terme dans les données qui rendent le problème d’optimisation beaucoup plus facile ». Nous copions cela en inversant la phrase allemande après qu’elle ait été transformée en une liste de jetons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings (tokens) and reverses it\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "orchText gère la façon dont les données doivent être traitées. Field\n",
    "\n",
    "Nous définissons l’argument sur la fonction de tokenisation correcte pour chacun, l’allemand étant le champ (source) et l’anglais le champ (cible). Le champ ajoute également les jetons « début de séquence » et « fin de séquence » via les arguments et, et convertit tous les mots en minuscules.tokenizeSRCTRGinit_tokeneos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/miniconda3/envs/pytorch17/lib/python3.8/site-packages/torchtext-0.9.0a0+c38fd42-py3.8-linux-x86_64.egg/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "SRC = Field(tokenize = tokenize_de, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous téléchargeons et chargeons les données du train, de la validation et des tests.\n",
    "\n",
    "Le jeu de données que nous utiliserons est le jeu de données Multi30k. Il s’agit d’un ensemble de données avec ~30 000 phrases parallèles en anglais, allemand et français, chacune avec ~12 mots par phrase.\n",
    "\n",
    "exts Spécifie les langues à utiliser comme source et cible (la source passe en premier) et spécifie le champ à utiliser pour la source et la cible.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/miniconda3/envs/pytorch17/lib/python3.8/site-packages/torchtext-0.9.0a0+c38fd42-py3.8-linux-x86_64.egg/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n",
    "                                                    fields = (SRC, TRG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons vérifier que nous avons chargé le bon nombre d’exemples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print out an example, making sure the source sentence is reversed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': ['.', 'büsche', 'vieler', 'nähe', 'der', 'in', 'freien', 'im', 'sind', 'männer', 'weiße', 'junge', 'zwei'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'src': ['.', 'büsche', 'vieler', 'nähe', 'der', 'in', 'freien', 'im', 'sind', 'männer', 'weiße', 'junge', 'zwei'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n",
    "Le point est au début de la phrase allemande (src), il semble donc que la phrase ait été correctement inversée.\n",
    "\n",
    "Ensuite, nous allons construire le vocabulaire pour les langues source et cible. Le vocabulaire est utilisé pour associer chaque jeton unique à un index (un entier). Les vocabulaires des langues source et cible sont distincts.\n",
    "\n",
    "En utilisant l’argument, nous n’autorisons que les jetons qui apparaissent au moins 2 fois à apparaître dans notre vocabulaire. Les jetons qui n’apparaissent qu’une seule fois sont convertis en jeton (inconnu).min_freq<unk>\n",
    "\n",
    "Il est important de noter que notre vocabulaire ne doit être construit qu’à partir de l’ensemble de formation et non de l’ensemble de validation/test. Cela empêche les « fuites d’informations » dans notre modèle, ce qui nous donne des scores de validation/test artificiellement gonflés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (de) vocabulary: 7853\n",
      "Unique tokens in target (en) vocabulary: 5893\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La dernière étape de la préparation des données consiste à créer les itérateurs. Ceux-ci peuvent être itérés pour renvoyer un lot de données qui aura un attribut (les tenseurs PyTorch contenant un lot de phrases sources numériques) et un attribut (les tenseurs PyTorch contenant un lot de phrases cibles numériques). Numérique est juste une façon sophistiquée de dire qu’ils ont été convertis d’une séquence de jetons lisibles en une séquence d’index correspondants, en utilisant le vocabulaire. srctrg\n",
    "\n",
    "Nous devons également définir un fichier . Ceci est utilisé pour dire à torchText de mettre les tenseurs sur le GPU ou non. Nous utilisons la fonction, qui reviendra si un GPU est détecté sur notre ordinateur. Nous transmettons cela à l’itérateur.torch.devicetorch.cuda.is_available()Truedevice\n",
    "\n",
    "Lorsque nous obtenons un lot d’exemples utilisant un itérateur, nous devons nous assurer que toutes les phrases sources sont complétées à la même longueur, la même chose que les phrases cibles. Heureusement, les itérateurs de torchText s’en chargent pour nous !\n",
    "\n",
    "Nous utilisons un au lieu de la norme car il crée des lots de telle sorte qu’il minimise la quantité de remplissage dans les phrases source et cible. BucketIteratorIterator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/miniconda3/envs/pytorch17/lib/python3.8/site-packages/torchtext-0.9.0a0+c38fd42-py3.8-linux-x86_64.egg/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction du  Seq2Seq Model\n",
    "\n",
    "Nous allons construire notre modèle en trois parties. L’encodeur, le décodeur et un modèle seq2seq qui encapsule l’encodeur et le décodeur et fournira un moyen d’interface avec chacun.\n",
    "\n",
    "### Encoder\n",
    "Tout d’abord, l’encodeur, un LSTM à 2 couches. Le papier que nous mettons en œuvre utilise un LSTM à 4 couches, mais dans l’intérêt du temps de formation, nous l’avons réduit à 2 couches. Le concept de RNN multicouche est facile à étendre de 2 à 4 couches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        #outputs = [src len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "Ensuite, nous allons construire notre décodeur, qui sera également un LSTM à 2 couches (4 dans le papier).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        \n",
    "        #input = [batch size]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #context = [n layers, batch size, hid dim]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "                \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        \n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #cell = [n layers, batch size, hid dim]\n",
    "        \n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq\n",
    "\n",
    "Pour la dernière partie de la mise en œuvre, nous allons implémenter le modèle seq2seq. Cela permettra de gérer :\n",
    "\n",
    "- réception de la phrase d’entrée/source\n",
    "- Utilisation de l’encodeur pour produire les vecteurs de contexte\n",
    "- Utilisation du décodeur pour produire la phrase de sortie/cible prévue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrainement du Seq2Seq Model\n",
    "\n",
    "Maintenant que notre modèle est mis en œuvre, nous pouvons commencer à le former.\n",
    "\n",
    "Tout d’abord, nous allons initialiser notre modèle. Comme mentionné précédemment, les dimensions d’entrée et de sortie sont définies par la taille du vocabulaire. Les dimesions d’incorporation et les abandons pour le codeur et le décodeur peuvent être différents, mais le nombre de couches et la taille des états masqués/cellulaires doivent être les mêmes.\n",
    "\n",
    "Nous définissons ensuite l’encodeur, le décodeur puis notre modèle Seq2Seq, que nous plaçons sur le fichier .device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La prochaine étape consiste à initialiser les poids de notre modèle. Dans l’article, ils indiquent qu’ils initialisent tous les poids à partir d’une distribution uniforme comprise entre -0,08 et +0,08, c’est-à-dire  (−0,08,0,08)\n",
    " .\n",
    "\n",
    "Nous initialisons les poids dans PyTorch en créant une fonction que nous à notre modèle. Lors de l’utilisation de , la fonction sera appelée sur chaque module et sous-module de notre modèle. Pour chaque module, nous parcourons en boucle tous les paramètres et les échantillonnons à partir d’une distribution uniforme avec .applyapplyinit_weightsnn.init.uniform_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(7853, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(5893, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=512, out_features=5893, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous définissons également une fonction qui calculera le nombre de paramètres pouvant être entraînés dans le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 13,898,501 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous définissons notre optimiseur, que nous utilisons pour mettre à jour nos paramètres dans la boucle d’entraînement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous définissons notre fonction de perte. La fonction calcule à la fois le log softmax et le log-vraisemblance négatif de nos prédictions. CrossEntropyLoss\n",
    "\n",
    "Notre fonction de perte calcule la perte moyenne par jeton, mais en passant l’index du jeton comme argument, nous ignorons la perte chaque fois que le jeton cible est un jeton de rembourrage. <pad>ignore_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous allons définir notre boucle d’entraînement.\n",
    "\n",
    "Tout d’abord, nous allons mettre le modèle en « mode d’entraînement » avec . Cela activera l’abandon (et la normalisation par lots, que nous n’utilisons pas), puis itérera dans notre itérateur de données.model.train()\n",
    "\n",
    "Enfin, nous retournons la perte qui est moyennée sur tous les lots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre boucle d’évaluation est similaire à notre boucle d’entraînement, mais comme nous ne mettons à jour aucun paramètre, nous n’avons pas besoin de passer un optimiseur ou une valeur de clip.\n",
    "\n",
    "N’oubliez pas de mettre le modèle en mode évaluation avec . Cela désactivera l’abandon (et la normalisation par lots, le cas échéant).model.eval()\n",
    "\n",
    "Nous utilisons le bloc pour nous assurer qu’aucun dégradé n’est calculé dans le bloc. Cela réduit la consommation de mémoire et accélère les choses. with torch.no_grad()\n",
    "\n",
    "La boucle d’itération est similaire (sans les mises à jour des paramètres), mais nous devons nous assurer que nous désactivons le forçage de l’enseignant pour l’évaluation. Cela entraînera le modèle à n’utiliser que ses propres prédictions pour faire d’autres prédictions dans une phrase, ce qui reflète la façon dont il serait utilisé dans le déploiement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous allons créer une fonction que nous utiliserons pour nous dire combien de temps dure une époque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons enfin commencer à former notre modèle!\n",
    "\n",
    "À chaque époque, nous vérifierons si notre modèle a obtenu la meilleure perte de validation jusqu’à présent. Si c’est le cas, nous mettrons à jour notre meilleure perte de validation et enregistrerons les paramètres de notre modèle (appelé dans PyTorch). Ensuite, lorsque nous viendrons tester notre modèle, nous utiliserons les paramètres enregistrés utilisés pour obtenir la meilleure perte de validation. state_dict\n",
    "\n",
    "Nous imprimerons à la fois la perte et la perplexité à chaque époque. Il est plus facile de voir un changement dans la perplexité qu’un changement dans la perte car les chiffres sont beaucoup plus importants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/miniconda3/envs/pytorch17/lib/python3.8/site-packages/torchtext-0.9.0a0+c38fd42-py3.8-linux-x86_64.egg/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 26s\n",
      "\tTrain Loss: 5.052 | Train PPL: 156.386\n",
      "\t Val. Loss: 4.916 |  Val. PPL: 136.446\n",
      "Epoch: 02 | Time: 0m 26s\n",
      "\tTrain Loss: 4.483 | Train PPL:  88.521\n",
      "\t Val. Loss: 4.789 |  Val. PPL: 120.154\n",
      "Epoch: 03 | Time: 0m 25s\n",
      "\tTrain Loss: 4.195 | Train PPL:  66.363\n",
      "\t Val. Loss: 4.552 |  Val. PPL:  94.854\n",
      "Epoch: 04 | Time: 0m 25s\n",
      "\tTrain Loss: 3.963 | Train PPL:  52.625\n",
      "\t Val. Loss: 4.485 |  Val. PPL:  88.672\n",
      "Epoch: 05 | Time: 0m 25s\n",
      "\tTrain Loss: 3.783 | Train PPL:  43.955\n",
      "\t Val. Loss: 4.375 |  Val. PPL:  79.466\n",
      "Epoch: 06 | Time: 0m 25s\n",
      "\tTrain Loss: 3.636 | Train PPL:  37.957\n",
      "\t Val. Loss: 4.234 |  Val. PPL:  69.011\n",
      "Epoch: 07 | Time: 0m 26s\n",
      "\tTrain Loss: 3.506 | Train PPL:  33.329\n",
      "\t Val. Loss: 4.077 |  Val. PPL:  58.948\n",
      "Epoch: 08 | Time: 0m 27s\n",
      "\tTrain Loss: 3.370 | Train PPL:  29.090\n",
      "\t Val. Loss: 4.018 |  Val. PPL:  55.581\n",
      "Epoch: 09 | Time: 0m 26s\n",
      "\tTrain Loss: 3.241 | Train PPL:  25.569\n",
      "\t Val. Loss: 3.934 |  Val. PPL:  51.113\n",
      "Epoch: 10 | Time: 0m 26s\n",
      "\tTrain Loss: 3.157 | Train PPL:  23.492\n",
      "\t Val. Loss: 3.927 |  Val. PPL:  50.743\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons charger les paramètres () qui ont donné à notre modèle la meilleure perte de validation et l’exécuter sur le jeu de test.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 3.951 | Test PPL:  52.001 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut1-model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
